{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])     \n",
    "# Download the dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # --- TODO: Normalize data manually to the range [0, 1] ---\n",
    "    # Normalize to [0, 1]\n",
    "#train_dataset.data = train_dataset.classes[0,1]\n",
    "#test_dataset.data = train_dataset.classes[0,1]\n",
    "    #subsample_50_percent_per_class = \n",
    "\n",
    "    # Subsampling: 50% from each class\n",
    "    #train_indices = subsample_50_percent_per_class(train_dataset)\n",
    "    #train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    # DataLoader for batching\n",
    "    #train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    #test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
      "           0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,   1,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,   0,\n",
      "          36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,   0,   3],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,\n",
      "         102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,  10,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,  72,  15],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,  69,\n",
      "         207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88, 172,  66],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0, 200,\n",
      "         232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196, 229,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 183,\n",
      "         225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245, 173,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 193,\n",
      "         228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243, 202,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12, 219,\n",
      "         220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197, 209,  52],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99, 244,\n",
      "         222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119, 167,  56],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55, 236,\n",
      "         228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,  92,   0],\n",
      "        [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237, 226,\n",
      "         217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,  77,   0],\n",
      "        [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228, 207,\n",
      "         213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244, 159,   0],\n",
      "        [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217, 226,\n",
      "         200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238, 215,   0],\n",
      "        [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200, 159,\n",
      "         245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232, 246,   0],\n",
      "        [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,  80,\n",
      "         150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228, 225,   0],\n",
      "        [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217, 241,\n",
      "          65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224, 229,  29],\n",
      "        [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198, 213,\n",
      "         240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221, 230,  67],\n",
      "        [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219, 221,\n",
      "         220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205, 206, 115],\n",
      "        [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211, 210,\n",
      "         200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177, 210,  92],\n",
      "        [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189, 188,\n",
      "         193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216, 170,   0],\n",
      "        [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244, 221,\n",
      "         220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
      "       dtype=torch.uint8)\n",
      "tensor(0, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.data[0])\n",
    "print(torch.min(train_dataset.data)) #ranges from 0- 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_dataset.data = train_dataset.data/255\n",
    "    test_dataset.data = test_dataset.data/255\n",
    "    print(torch.min(train_dataset.data))\n",
    "    print(torch.max(train_dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FashionMNIST dataset \n",
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  # Only convert to tensor\n",
    "\n",
    "    # Download the dataset\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # --- TODO: Normalize data manually to the range [0, 1] ---\n",
    "    # Normalize to [0, 1]\n",
    "    train_dataset.data = train_dataset.data/255\n",
    "    test_dataset.data = test_dataset.data/255\n",
    "    print(torch.min(train_dataset.data))\n",
    "    print(torch.max(train_dataset.data))\n",
    "    subsample_50_percent_per_class = train_dataset\n",
    "\n",
    "    # Subsampling: 50% from each class\n",
    "    train_indices = subsample_50_percent_per_class(train_dataset)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    # DataLoader for batching\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;241m64\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmax(train_dataset\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#subsample_50_percent_per_class = \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Subsampling: 50% from each class\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#train_indices = subsample_50_percent_per_class(train_dataset)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_subset \u001b[38;5;241m=\u001b[39m Subset(train_dataset, train_indices)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# DataLoader for batching\u001b[39;00m\n\u001b[1;32m     22\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_subset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_indices' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FashionMNIST dataset \n",
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  # Only convert to tensor\n",
    "\n",
    "    # Download the dataset\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # --- TODO: Normalize data manually to the range [0, 1] ---\n",
    "    # Normalize to [0, 1]\n",
    "    train_dataset.data = \n",
    "    test_dataset.data = \n",
    "\n",
    "    # Subsampling: 50% from each class\n",
    "    train_indices = subsample_50_percent_per_class(train_dataset)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    # DataLoader for batching\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Function to perform subsampling 50% from each class\n",
    "def subsample_50_percent_per_class(dataset):\n",
    "    \"\"\"\n",
    "    Subsample 50% of the data from each class.\n",
    "    dataset: The full dataset (e.g., FashionMNIST)\n",
    "    Returns: A list of indices for the subsampled dataset\n",
    "    \"\"\"\n",
    "    # --- TODO: Implement subsampling logic here ---\n",
    "    sampled_indices = []\n",
    "    num_classes = torch. #loop over each of the 9 classes & take 50% of \n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# Forward pass for Fully Connected Layer\n",
    "def fully_connected_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Perform forward pass for a fully connected (linear) layer.\n",
    "    X: Input data\n",
    "    W: Weight matrix\n",
    "    b: Bias vector\n",
    "    \"\"\"\n",
    "    Z = None  # TODO: Compute the linear transformation (X * W + b) Z = wX + b\n",
    "    return Z\n",
    "\n",
    "# Forward pass for ReLU activation\n",
    "def relu_forward(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function forward pass.\n",
    "    Z: Linear output (input to ReLU)\n",
    "    \"\"\"\n",
    "    A = None  # TODO: Apply ReLU function (element-wise)\n",
    "    return A\n",
    "\n",
    "# Forward pass for Softmax activation\n",
    "def softmax_forward(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function forward pass.\n",
    "    Z: Output logits (before softmax)\n",
    "    \"\"\"\n",
    "    exp_z = None  # TODO: Apply softmax function (numerical stability)\n",
    "    output = None  # TODO: Normalize exp_z to get the softmax output\n",
    "    return output\n",
    "\n",
    "# Backward pass for Fully Connected Layer (Linear)\n",
    "def fully_connected_backward(X, Z, W, Y):\n",
    "    \"\"\"\n",
    "    Compute gradients for the fully connected (linear) layer.\n",
    "    X: Input data\n",
    "    Z: Output of the layer before activation (logits)\n",
    "    W: Weight matrix\n",
    "    Y: True labels (for loss gradient calculation)\n",
    "    \"\"\"\n",
    "    dW = None  # TODO: Compute gradient of weights (X^T * dZ)\n",
    "    db = None  # TODO: Compute gradient of bias (sum of dZ)\n",
    "    dZ = None  # TODO: Compute gradient of loss with respect to Z (for backpropagation)\n",
    "    return dW, db, dZ\n",
    "\n",
    "# Backward pass for ReLU activation\n",
    "def relu_backward(Z, dA):\n",
    "    \"\"\"\n",
    "    Compute the gradient for ReLU activation.\n",
    "    Z: Input to ReLU (before activation)\n",
    "    dA: Gradient of the loss with respect to activations (from the next layer)\n",
    "    \"\"\"\n",
    "    dZ = None  # TODO: Compute dZ for ReLU (gradient is 0 for Z <= 0 and dA for Z > 0)\n",
    "    return dZ\n",
    "\n",
    "# Backward pass for Softmax Layer\n",
    "def softmax_backward(Z, Y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to softmax output.\n",
    "    Z: Output logits (before softmax)\n",
    "    Y: True labels (one-hot encoded)\n",
    "    \"\"\"\n",
    "    dZ = None  # TODO: Compute dZ for softmax (Z - Y)\n",
    "    return dZ\n",
    "\n",
    "# Weight update function (gradient descent)\n",
    "def update_weights(weights, biases, grads_W, grads_b, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    --- TODO: Implement the weight update step ---\n",
    "    weights: Current weights\n",
    "    biases: Current biases\n",
    "    grads_W: Gradient of the weights\n",
    "    grads_b: Gradient of the biases\n",
    "    learning_rate: Learning rate for gradient descent\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network \n",
    "def train(train_loader, test_loader, epochs=10000, learning_rate=0.01):\n",
    "    # Initialize weights and biases\n",
    "    input_dim = #TODO\n",
    "    hidden_dim1 = 128   #could set differently\n",
    "    hidden_dim2 = 64    #could set differently\n",
    "    output_dim = # TODO\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    W1 = torch.randn(input_dim, hidden_dim1, requires_grad=False) * 0.01\n",
    "    b1 = torch.zeros(hidden_dim1, requires_grad=False)\n",
    "    W2 = #TODO\n",
    "    b2 = #TODO\n",
    "    W3 = #TODO\n",
    "    b3 = #TODO\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "            # Flatten images to vectors\n",
    "            X_batch = #TODO  # Flatten images to vector\n",
    "            Y_batch = torch.eye(output_dim)[Y_batch]  # Map label indices to corresponding one-hot encoded vectors\n",
    "\n",
    "            # --- TODO: Implement the forward pass ---\n",
    "            Z1 = #TODO\n",
    "            A1 = #TODO\n",
    "            Z2 = #TODO\n",
    "            A2 = #TODO\n",
    "            Z3 = #TODO\n",
    "            Y_pred = #TODO\n",
    "            \n",
    "            # --- TODO: Implement loss computation ---\n",
    "            # loss = ___\n",
    "\n",
    "            epoch_loss = #TODO\n",
    "\n",
    "            # --- TODO: Implement backward pass ---\n",
    "            dZ3 = #TODO\n",
    "            dW3, db3, dA2 = #TODO\n",
    "            dZ2 = #TODO\n",
    "            dW2, db2, dA1 = #TODO\n",
    "            dZ1 = #TODO\n",
    "            dW1, db1, _ = #TODO\n",
    "\n",
    "            # --- TODO: Implement weight update ---\n",
    "            W1, b1 = update_weights(W1, b1, dW1, db1, learning_rate)\n",
    "            W2, b2 = update_weights(W2, b2, dW2, db2, learning_rate)\n",
    "            W3, b3 = update_weights(W3, b3, dW3, db3, learning_rate)\n",
    "\n",
    "            # Track accuracy\n",
    "            correct_predictions = #TODO\n",
    "            total_samples = #TODO\n",
    "\n",
    "        # Print out the progress\n",
    "        train_accuracy = correct_predictions / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}, Accuracy: {train_accuracy * 100}%\")\n",
    "\n",
    "        # TODO: For every 1000 epochs, get the validation loss and error\n",
    "        \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    batch_size = 64\n",
    "    train_loader, test_loader = load_data(batch_size)\n",
    "\n",
    "    # Start training\n",
    "    train(train_loader, test_loader, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

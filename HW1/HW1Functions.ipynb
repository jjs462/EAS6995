{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW 1 \n",
    "\n",
    "Jessica Smith\n",
    "\n",
    "Full function code block- first is the [in progress] completed, second is the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''\n",
    "Functions to look at that may be useful:\n",
    "np.sum()\n",
    "np.where()\n",
    "np.maximum()\n",
    "np.log()\n",
    "np.exp()\n",
    "np.argmax()\n",
    "np.dot()\n",
    ".append()\n",
    "np.random.choice()\n",
    "\n",
    "For torch tensors:\n",
    "X.view()\n",
    "X.numpy()\n",
    "X.item()\n",
    "dataset.targets\n",
    "dataset.data\n",
    "'''\n",
    "\n",
    "# Load the FashionMNIST dataset \n",
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  # Only convert to tensor\n",
    "\n",
    "    # Download the dataset\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Normalize data manually to the range [0, 1] ---\n",
    "    train_dataset.data = train_dataset.data/255\n",
    "    test_dataset.data = test_dataset.data/255\n",
    "    print(torch.min(train_dataset.data))\n",
    "    print(torch.max(train_dataset.data))\n",
    "\n",
    "    # Subsampling: 50% from each class\n",
    "    train_indices = subsample_50_percent_per_class(train_dataset)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    # DataLoader for batching\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Function to perform subsampling 50% from each class\n",
    "def subsample_50_percent_per_class(dataset):\n",
    "    \"\"\"\n",
    "    Subsample 50% of the data from each class.\n",
    "    dataset: The full dataset (e.g., FashionMNIST)\n",
    "    Returns: A list of indices for the subsampled dataset\n",
    "    \"\"\"\n",
    "    # Implement subsampling logic here ---\n",
    "    sampled_indices = []\n",
    "    #num_classes = torch. #loop over each of the 10 classes & take 50% of \n",
    "    num_classes = 10  # FashionMNIST has 10 classes\n",
    "    \n",
    "    targets = dataset.targets.clone().detach()  # Convert to tensor if not already\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        # Get all indices for the current class\n",
    "        class_indices = torch.where(targets == class_id)[0]\n",
    "\n",
    "        # Shuffle indices and select 50%\n",
    "        num_samples = len(class_indices) // 2  # Take half\n",
    "        selected_indices = class_indices[torch.randperm(len(class_indices))[:num_samples]]\n",
    "\n",
    "        # Store the selected indices\n",
    "        sampled_indices.extend(selected_indices.tolist())\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# Forward pass for Fully Connected Layer\n",
    "def fully_connected_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Perform forward pass for a fully connected (linear) layer.\n",
    "    X: Input data\n",
    "    W: Weight matrix\n",
    "    b: Bias vector\n",
    "    \"\"\"\n",
    "    Z = None  # TODO: Compute the linear transformation (X * W + b)\n",
    "    return Z\n",
    "\n",
    "# Forward pass for ReLU activation\n",
    "def relu_forward(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function forward pass.\n",
    "    Z: Linear output (input to ReLU)\n",
    "    \"\"\"\n",
    "    A = None  # TODO: Apply ReLU function (element-wise)\n",
    "    return A\n",
    "\n",
    "# Forward pass for Softmax activation\n",
    "def softmax_forward(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function forward pass.\n",
    "    Z: Output logits (before softmax)\n",
    "    \"\"\"\n",
    "    exp_z = None  # TODO: Apply softmax function (numerical stability)\n",
    "    output = None  # TODO: Normalize exp_z to get the softmax output\n",
    "    return output\n",
    "\n",
    "# Backward pass for Fully Connected Layer (Linear)\n",
    "def fully_connected_backward(X, Z, W, dZ):\n",
    "    \"\"\"\n",
    "    NOTE CLARIFICATION HERE; dZ is an input instead of Y\n",
    "    Compute gradients for the fully connected (linear) layer.\n",
    "    X: Input data (Nxd)\n",
    "    Z: Output of the layer before activation (logits, NxK)\n",
    "    W: Weight matrix (dxK)\n",
    "    dZ: Gradient of the loss with respect to Z (from the next layer)\n",
    "    \"\"\"\n",
    "    dW = None  # TODO: Compute gradient of weights (X^T * dZ)\n",
    "    db = None  # TODO: Compute gradient of bias (sum of dZ)\n",
    "    dZ = None  # TODO: Compute gradient of loss with respect to Z (for backpropagation)\n",
    "    return dW, db, dZ\n",
    "\n",
    "# Backward pass for ReLU activation\n",
    "def relu_backward(Z, dA):\n",
    "    \"\"\"\n",
    "    Compute the gradient for ReLU activation.\n",
    "    Z: Input to ReLU (before activation)\n",
    "    dA: Gradient of the loss with respect to activations (from the next layer)\n",
    "    \"\"\"\n",
    "    dZ = None  # TODO: Compute dZ for ReLU (gradient is 0 for Z <= 0 and dA for Z > 0)\n",
    "    return dZ\n",
    "\n",
    "# Backward pass for Softmax Layer\n",
    "def softmax_backward(S, Y):\n",
    "    \"\"\"\n",
    "    NOTE THE CORRECTION/EFFICIENCY GAIN HERE in using softmax output instead of Z\n",
    "    Compute the gradient of the loss with respect to softmax output.\n",
    "    S: Output of softmax \n",
    "    Y: True labels (one-hot encoded)\n",
    "    \"\"\"\n",
    "    dZ = None  # TODO: Compute dZ for softmax (S - Y)\n",
    "    return dZ\n",
    "\n",
    "# Weight update function (gradient descent)\n",
    "def update_weights(weights, biases, grads_W, grads_b, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    --- TODO: Implement the weight update step ---\n",
    "    weights: Current weights\n",
    "    biases: Current biases\n",
    "    grads_W: Gradient of the weights\n",
    "    grads_b: Gradient of the biases\n",
    "    learning_rate: Learning rate for gradient descent\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Define the neural network \n",
    "def train(train_loader, test_loader, epochs=10000, learning_rate=0.01):\n",
    "    # Initialize weights and biases\n",
    "    input_dim = #TODO\n",
    "    hidden_dim1 = 128   #could set differently\n",
    "    hidden_dim2 = 64    #could set differently\n",
    "    output_dim = # TODO\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    # NOTE THE CORRECTION HERE! I HAD it done using torch but needs to be numpy\n",
    "    # Note also that this is not using the specific methods I had mentioned for\n",
    "    #   weight initialization (e.g. Xavier or He), this is just random\n",
    "    W1 = np.random.randn(input_dim, hidden_dim1) * 0.01\n",
    "    b1 = np.zeros(hidden_dim1)\n",
    "    W2 = #TODO\n",
    "    b2 = #TODO\n",
    "    W3 = #TODO\n",
    "    b3 = #TODO\n",
    "    \n",
    "    # ADD THESE to save training and test loss, accuracy\n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        test_epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "            # Flatten images to vectors\n",
    "            X_batch = #TODO  # Flatten images to vector\n",
    "            Y_batch = torch.eye(output_dim)[Y_batch]  # Map label indices to corresponding one-hot encoded vectors\n",
    "            \n",
    "            # CONVERT TORCH TENSORS to numpy\n",
    "            X = # TODO\n",
    "            y = # TODO\n",
    "\n",
    "            # --- TODO: Implement the forward pass ---\n",
    "            Z1 = #TODO\n",
    "            A1 = #TODO\n",
    "            Z2 = #TODO\n",
    "            A2 = #TODO\n",
    "            Z3 = #TODO\n",
    "            Y_pred = #TODO\n",
    "            \n",
    "            # --- TODO: Implement loss computation ---\n",
    "            loss = ___\n",
    "\n",
    "            epoch_loss = #TODO\n",
    "\n",
    "            # --- TODO: Implement backward pass ---\n",
    "            dZ3 = #TODO\n",
    "            dW3, db3, dA2 = #TODO\n",
    "            dZ2 = #TODO\n",
    "            dW2, db2, dA1 = #TODO\n",
    "            dZ1 = #TODO\n",
    "            dW1, db1, dX = #TODO\n",
    "\n",
    "            # --- TODO: Implement weight update ---\n",
    "            W1, b1 = update_weights(W1, b1, dW1, db1, learning_rate)\n",
    "            W2, b2 = update_weights(W2, b2, dW2, db2, learning_rate)\n",
    "            W3, b3 = update_weights(W3, b3, dW3, db3, learning_rate)\n",
    "\n",
    "            # Track accuracy\n",
    "            correct_predictions = #for this batch; TODO\n",
    "            total_correct_predictions = #for the entire epoch; TODO\n",
    "            total_samples = #for entire epoch; TODO\n",
    "\n",
    "        # Print out the progress - CLARIFIED\n",
    "        train_accuracy = total_correct_predictions / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}, Accuracy: {train_accuracy * 100}%\")\n",
    "        \n",
    "        # Save the training loss and accuracy for each epoch to plot later\n",
    "\n",
    "        # TODO: For every 100 epochs, get the validation loss and error\n",
    "        # FREQUENCY OF THIS IS CHANGED FROM EVERY 1000 to EVERY 100\n",
    "        \n",
    "        # Save the test loss and accuracy for every 100th epoch to plot later\n",
    "        \n",
    "    return training_loss, training_accuracy, test_loss, test_accuracy\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    batch_size = 64\n",
    "    train_loader, test_loader = load_data(batch_size)\n",
    "\n",
    "    # Start training\n",
    "    training_loss, training_accuracy, test_loss, test_accuracy = train(train_loader, test_loader, epochs=1000, learning_rate=0.1)\n",
    "    \n",
    "    \n",
    "    # PLOT TRAINING LOSS AND TEST LOSS ON ONE SUBPLOT (epoch vs loss)\n",
    "    # PLOT TRAINING ACCURACY AND TEST ACCURACY ON A SECOND SUBPLOT (epoch vs accuracy)\n",
    "    \n",
    "    epochs_train = list(range(1, len(training_loss) + 1))  # Epochs for training loss (1, 2, ..., N)\n",
    "    epochs_test = list(range(100, (len(test_loss) + 1) * 100, 100))  # Epochs for test loss (100, 200, ..., N*100)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot Training and Test Loss on the first subplot\n",
    "    ax1.plot(..., ..., label='Training Loss', color='blue', marker='o')\n",
    "    ax1.plot(..., ..., label='Test Loss', color='red', marker='x')\n",
    "    ax1.set_title('Loss vs Epoch')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot Training and Test Accuracy on the second subplot\n",
    "    ax2.plot(..., ..., label='Training Accuracy', color='blue', marker='o')\n",
    "    ax2.plot(..., ..., label='Test Accuracy', color='red', marker='x')\n",
    "    ax2.set_title('Accuracy vs Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''\n",
    "Functions to look at that may be useful:np.sum()\n",
    "np.where()\n",
    "np.maximum()\n",
    "np.log()\n",
    "np.exp()\n",
    "np.argmax()\n",
    "np.dot()\n",
    ".append()\n",
    "np.random.choice()\n",
    "\n",
    "For torch tensors:\n",
    "X.view()\n",
    "X.numpy()\n",
    "X.item()\n",
    "dataset.targets\n",
    "dataset.data\n",
    "'''\n",
    "\n",
    "# Load the FashionMNIST dataset \n",
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  # Only convert to tensor\n",
    "\n",
    "    # Download the dataset\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # --- TODO: Normalize data manually to the range [0, 1] ---\n",
    "    # Normalize to [0, 1]\n",
    "    # train_dataset.data = ___\n",
    "    # test_dataset.data = ___\n",
    "\n",
    "    # Subsampling: 50% from each class\n",
    "    train_indices = subsample_50_percent_per_class(train_dataset)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    # DataLoader for batching\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Function to perform subsampling 50% from each class\n",
    "def subsample_50_percent_per_class(dataset):\n",
    "    \"\"\"\n",
    "    Subsample 50% of the data from each class.\n",
    "    dataset: The full dataset (e.g., FashionMNIST)\n",
    "    Returns: A list of indices for the subsampled dataset\n",
    "    \"\"\"\n",
    "    # --- TODO: Implement subsampling logic here ---\n",
    "    sampled_indices = []\n",
    "\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# Forward pass for Fully Connected Layer\n",
    "def fully_connected_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Perform forward pass for a fully connected (linear) layer.\n",
    "    X: Input data\n",
    "    W: Weight matrix\n",
    "    b: Bias vector\n",
    "    \"\"\"\n",
    "    Z = None  # TODO: Compute the linear transformation (X * W + b)\n",
    "    return Z\n",
    "\n",
    "# Forward pass for ReLU activation\n",
    "def relu_forward(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function forward pass.\n",
    "    Z: Linear output (input to ReLU)\n",
    "    \"\"\"\n",
    "    A = None  # TODO: Apply ReLU function (element-wise)\n",
    "    return A\n",
    "\n",
    "# Forward pass for Softmax activation\n",
    "def softmax_forward(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function forward pass.\n",
    "    Z: Output logits (before softmax)\n",
    "    \"\"\"\n",
    "    exp_z = None  # TODO: Apply softmax function (numerical stability)\n",
    "    output = None  # TODO: Normalize exp_z to get the softmax output\n",
    "    return output\n",
    "\n",
    "# Backward pass for Fully Connected Layer (Linear)\n",
    "def fully_connected_backward(X, Z, W, dZ):\n",
    "    \"\"\"\n",
    "    NOTE CLARIFICATION HERE; dZ is an input instead of Y\n",
    "    Compute gradients for the fully connected (linear) layer.\n",
    "    X: Input data (Nxd)\n",
    "    Z: Output of the layer before activation (logits, NxK)\n",
    "    W: Weight matrix (dxK)\n",
    "    dZ: Gradient of the loss with respect to Z (from the next layer)\n",
    "    \"\"\"\n",
    "    dW = None  # TODO: Compute gradient of weights (X^T * dZ)\n",
    "    db = None  # TODO: Compute gradient of bias (sum of dZ)\n",
    "    dZ = None  # TODO: Compute gradient of loss with respect to Z (for backpropagation)\n",
    "    return dW, db, dZ\n",
    "\n",
    "# Backward pass for ReLU activation\n",
    "def relu_backward(Z, dA):\n",
    "    \"\"\"\n",
    "    Compute the gradient for ReLU activation.\n",
    "    Z: Input to ReLU (before activation)\n",
    "    dA: Gradient of the loss with respect to activations (from the next layer)\n",
    "    \"\"\"\n",
    "    dZ = None  # TODO: Compute dZ for ReLU (gradient is 0 for Z <= 0 and dA for Z > 0)\n",
    "    return dZ\n",
    "\n",
    "# Backward pass for Softmax Layer\n",
    "def softmax_backward(S, Y):\n",
    "    \"\"\"\n",
    "    NOTE THE CORRECTION/EFFICIENCY GAIN HERE in using softmax output instead of Z\n",
    "    Compute the gradient of the loss with respect to softmax output.\n",
    "    S: Output of softmax \n",
    "    Y: True labels (one-hot encoded)\n",
    "    \"\"\"\n",
    "    dZ = None  # TODO: Compute dZ for softmax (S - Y)\n",
    "    return dZ\n",
    "\n",
    "# Weight update function (gradient descent)\n",
    "def update_weights(weights, biases, grads_W, grads_b, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    --- TODO: Implement the weight update step ---\n",
    "    weights: Current weights\n",
    "    biases: Current biases\n",
    "    grads_W: Gradient of the weights\n",
    "    grads_b: Gradient of the biases\n",
    "    learning_rate: Learning rate for gradient descent\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Define the neural network \n",
    "def train(train_loader, test_loader, epochs=10000, learning_rate=0.01):\n",
    "    # Initialize weights and biases\n",
    "    input_dim = #TODO\n",
    "    hidden_dim1 = 128   #could set differently\n",
    "    hidden_dim2 = 64    #could set differently\n",
    "    output_dim = # TODO\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    # NOTE THE CORRECTION HERE! I HAD it done using torch but needs to be numpy\n",
    "    # Note also that this is not using the specific methods I had mentioned for\n",
    "    #   weight initialization (e.g. Xavier or He), this is just random\n",
    "    W1 = np.random.randn(input_dim, hidden_dim1) * 0.01\n",
    "    b1 = np.zeros(hidden_dim1)\n",
    "    W2 = #TODO\n",
    "    b2 = #TODO\n",
    "    W3 = #TODO\n",
    "    b3 = #TODO\n",
    "    \n",
    "    # ADD THESE to save training and test loss, accuracy\n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        test_epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "            # Flatten images to vectors\n",
    "            X_batch = #TODO  # Flatten images to vector\n",
    "            Y_batch = torch.eye(output_dim)[Y_batch]  # Map label indices to corresponding one-hot encoded vectors\n",
    "            \n",
    "            # CONVERT TORCH TENSORS to numpy\n",
    "            X = # TODO\n",
    "            y = # TODO\n",
    "\n",
    "            # --- TODO: Implement the forward pass ---\n",
    "            Z1 = #TODO\n",
    "            A1 = #TODO\n",
    "            Z2 = #TODO\n",
    "            A2 = #TODO\n",
    "            Z3 = #TODO\n",
    "            Y_pred = #TODO\n",
    "            \n",
    "            # --- TODO: Implement loss computation ---\n",
    "            loss = ___\n",
    "\n",
    "            epoch_loss = #TODO\n",
    "\n",
    "            # --- TODO: Implement backward pass ---\n",
    "            dZ3 = #TODO\n",
    "            dW3, db3, dA2 = #TODO\n",
    "            dZ2 = #TODO\n",
    "            dW2, db2, dA1 = #TODO\n",
    "            dZ1 = #TODO\n",
    "            dW1, db1, dX = #TODO\n",
    "\n",
    "            # --- TODO: Implement weight update ---\n",
    "            W1, b1 = update_weights(W1, b1, dW1, db1, learning_rate)\n",
    "            W2, b2 = update_weights(W2, b2, dW2, db2, learning_rate)\n",
    "            W3, b3 = update_weights(W3, b3, dW3, db3, learning_rate)\n",
    "\n",
    "            # Track accuracy\n",
    "            correct_predictions = #for this batch; TODO\n",
    "            total_correct_predictions = #for the entire epoch; TODO\n",
    "            total_samples = #for entire epoch; TODO\n",
    "\n",
    "        # Print out the progress - CLARIFIED\n",
    "        train_accuracy = total_correct_predictions / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}, Accuracy: {train_accuracy * 100}%\")\n",
    "        \n",
    "        # Save the training loss and accuracy for each epoch to plot later\n",
    "\n",
    "        # TODO: For every 100 epochs, get the validation loss and error\n",
    "        # FREQUENCY OF THIS IS CHANGED FROM EVERY 1000 to EVERY 100\n",
    "        \n",
    "        # Save the test loss and accuracy for every 100th epoch to plot later\n",
    "        \n",
    "    return training_loss, training_accuracy, test_loss, test_accuracy\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    batch_size = 64\n",
    "    train_loader, test_loader = load_data(batch_size)\n",
    "\n",
    "    # Start training\n",
    "    training_loss, training_accuracy, test_loss, test_accuracy = train(train_loader, test_loader, epochs=1000, learning_rate=0.1)\n",
    "    \n",
    "    \n",
    "    # PLOT TRAINING LOSS AND TEST LOSS ON ONE SUBPLOT (epoch vs loss)\n",
    "    # PLOT TRAINING ACCURACY AND TEST ACCURACY ON A SECOND SUBPLOT (epoch vs accuracy)\n",
    "    \n",
    "    epochs_train = list(range(1, len(training_loss) + 1))  # Epochs for training loss (1, 2, ..., N)\n",
    "    epochs_test = list(range(100, (len(test_loss) + 1) * 100, 100))  # Epochs for test loss (100, 200, ..., N*100)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot Training and Test Loss on the first subplot\n",
    "    ax1.plot(..., ..., label='Training Loss', color='blue', marker='o')\n",
    "    ax1.plot(..., ..., label='Test Loss', color='red', marker='x')\n",
    "    ax1.set_title('Loss vs Epoch')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot Training and Test Accuracy on the second subplot\n",
    "    ax2.plot(..., ..., label='Training Accuracy', color='blue', marker='o')\n",
    "    ax2.plot(..., ..., label='Test Accuracy', color='red', marker='x')\n",
    "    ax2.set_title('Accuracy vs Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
